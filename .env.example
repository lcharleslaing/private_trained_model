# ============================================
# Private GPT-OSS Chat Interface Configuration
# ============================================
# Copy this file to .env and edit as needed
# cp .env.example .env

# --------------------------------------------
# Ollama Configuration
# --------------------------------------------
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2

# --------------------------------------------
# Server Configuration
# --------------------------------------------
BACKEND_HOST=0.0.0.0
BACKEND_PORT=8000
FRONTEND_PORT=5173

# --------------------------------------------
# CORS Configuration (comma-separated origins)
# --------------------------------------------
CORS_ORIGINS=http://localhost:5173,http://localhost:3000

# --------------------------------------------
# Document Processing Configuration
# --------------------------------------------
# Supported file types:
#   - Documents: PDF, Word (DOCX, DOC), Excel (XLSX, XLS), TXT
#   - Images: JPG, PNG, GIF, BMP, TIFF, WEBP
DOCUMENTS_DIR=documents
EMBEDDINGS_DIR=embeddings
EMBEDDING_MODEL=all-MiniLM-L6-v2
CHUNK_SIZE=500
CHUNK_OVERLAP=50

# --------------------------------------------
# OCR Configuration
# --------------------------------------------
# Enable OCR to extract text from:
#   - Image files (JPG, PNG, GIF, etc.)
#   - Scanned PDFs and drawings
#   - Image-based PDFs
# Requires: easyocr, pdf2image, and Poppler installed
# First run downloads EasyOCR models (~100MB, one-time, requires internet)
# See OCR_SETUP.md for installation instructions
ENABLE_OCR=true

# --------------------------------------------
# Vision Model Configuration
# --------------------------------------------
# Enable vision model to understand visual content in:
#   - Image files (JPG, PNG, GIF, etc.)
#   - PDFs with drawings/images
#   - Scanned documents
# Requires: Ollama vision model installed (e.g., llava, bakllava)
# Install with: ollama pull llava
# First run downloads model (~4-7GB, one-time, requires internet)
# See VISION_MODEL_SETUP.md for detailed instructions
ENABLE_VISION=true
OLLAMA_VISION_MODEL=llava

# --------------------------------------------
# RAG (Retrieval Augmented Generation) Configuration
# --------------------------------------------
# Number of document chunks to retrieve per query
RAG_TOP_K=3

# Similarity threshold (0.0-1.0)
# Lower = more strict (only very relevant docs)
# Higher = more lenient (allows less relevant docs)
# Recommended values:
#   - Very strict: 0.2-0.25
#   - Balanced: 0.3 (default)
#   - Lenient: 0.4-0.5
SIMILARITY_THRESHOLD=0.3

# --------------------------------------------
# Optional: Logging
# --------------------------------------------
LOG_LEVEL=INFO
